{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb258b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db28dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\", \n",
    "                     model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53042b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'multi_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/base.py:1100\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1097\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1098\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[0;32m-> 1100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:19\u001b[0m, in \u001b[0;36mPipelineDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     18\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[i]\n\u001b[0;32m---> 19\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:179\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2548\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2635\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2636\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2651\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2652\u001b[0m     )\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2727\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2718\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2719\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2720\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2725\u001b[0m )\n\u001b[0;32m-> 2727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/mito/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    480\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    498\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    499\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 500\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'multi_label'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts = [\"Das funktioniert überhaupt nicht.\",\n",
    "         \"Ich bin absolut begeistert!\",\n",
    "         \"Works like a charm.\",\n",
    "         \"J'aime beaucoup.\",\n",
    "         \"Der letzte Müll!\"]\n",
    "output = sentiment(texts, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fe380b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0cc3c51b3ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(output)\n",
    "df[\"text\"] = texts\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install readability-lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a masked-language model published in 2018 by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in NLP experiments\", counting over 150 research publications analyzing and improving the model.[3]\n",
    "\n",
    "BERT was originally implemented in the English language at two model sizes:[1] (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BooksCorpus[4] (800M words) and English Wikipedia (2,500M words).\n",
    "\n",
    "Architecture\n",
    "BERT is based on the transformer architecture. Specifically, BERT is composed of Transformer encoder layers.\n",
    "\n",
    "BERT was pre-trained simultaneously on two tasks: language modeling (15% of tokens were masked, and the training objective was to predict the original token given its context) and next sentence prediction (the training objective was to classify if two spans of text appeared sequentially in the training corpus).[5] As a result of this training process, BERT learns latent representations of words and sentences in context. After pre-training, BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation).[1][6] The pre-training stage is significantly more computationally expensive than finetuning.\n",
    "\n",
    "Performance\n",
    "When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:[1]\n",
    "\n",
    "GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)\n",
    "SQuAD (Stanford Question Answering Dataset[7]) v1.1 and v2.0\n",
    "SWAG (Situations With Adversarial Generations[8])\n",
    "Analysis\n",
    "The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood.[9][10] Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences,[11][12] analysis of internal vector representations through probing classifiers,[13][14] and the relationships represented by attention weights.[9][10] The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.\n",
    "\n",
    "In contrast to deep learning neural networks which require very large amounts of data, BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example, if the task is a sentiment classification task on financial data, a pre-trained model for the analysis of sentiment of financial text should be chosen. The pre-trained models can be found in the Hugging Face library.\n",
    "\n",
    "History\n",
    "BERT has its origins from pre-training contextual representations, including semi-supervised sequence learning,[15] generative pre-training, ELMo,[16] and ULMFit.[17] Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, where BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\n",
    "\n",
    "On October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US.[18] On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages.[19] In October 2020, almost every single English-based query was processed by a BERT model.[20]\n",
    "\n",
    "Recognition\n",
    "The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).[21]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "doc = Document(requests.get(\"https://en.wikipedia.org/wiki/BERT_(language_model)\", \n",
    "                            stream=True).text)\n",
    "soup = BeautifulSoup(doc.summary())            \n",
    "content = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = summarizer(content[0:4000])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903ff5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pipeline(\"text-generation\", model=\"dbmdz/german-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(writer(\"Heute war ein so schöner Tag\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910759e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf962d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_shot = pipeline(\"zero-shot-classification\", \n",
    "                     model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3423c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aus https://www.heise.de/news/Landgericht-Leipzig-macht-DNS-Betreiber-zu-Taeter-von-Urheberrechtsverletzungen-7532128.html\n",
    "text = \"\"\"\n",
    "Landgericht Leipzig macht DNS-Betreiber zu Täter von Urheberrechtsverletzungen\n",
    "\n",
    "Betreiber von DNS-Resolvern sollen etwaige Weigerungen, mutmaßlich \n",
    "urheberrechtsverletzende Seiten zu sperren, mit hohen Geld- oder Haftstrafen bezahlen.\n",
    "\n",
    "Nicht als Störer, sondern als Täter der Urheberrechtsverletzung hat das Landgericht \n",
    "Leipzig den in Zürich ansässigen DNS-Dienst Quad9 verurteilt. Bei Androhung von bis \n",
    "zu 250.000 Euro Ordnungsgeld beziehungsweise bis zu 2 Jahren Haft verbietet es dem \n",
    "kleinen Resolver-Betreiber, zwei verwandte Domains in die dazugehörigen IP-Adressen \n",
    "zu übersetzen. Über diese Domains können Nutzer die über Shareplace.org angebotenen \n",
    "Titel eines Sony-Musikalbums finden.\n",
    "\n",
    "Quad9, dessen nicht-kommerziell agierende Betreiber-Stiftung vor wenigen Jahren in \n",
    "die Schweiz übersiedelt ist, will mit ihrem öffentlichen DNS Resolver eine \n",
    "Alternative zu Google und Cloudflare anbieten. Sony-Anwälte erwirkten im Frühjahr \n",
    "2021 vor dem Landgericht Hamburg eine einstweilige Verfügung gegen Quad9, um den \n",
    "Resolver-Betreiber zur Sperrung der beiden Domains zu zwingen.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\"Politik\", \"Wirtschaft\", \"Technik\", \"Recht\", \"Aktien\", \"Kultur\"]\n",
    "output = classifier(text, candidate_labels, multi_label=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a1537",
   "metadata": {},
   "source": [
    "# Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90ba96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7553\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7369\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6433\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.2564\n",
      "A man is eating food. \t A man is riding a horse. \t 0.2474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c3b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"\n",
    "Ich habe großen Hunger.\n",
    "Ich würde jetzt gern was essen.\n",
    "Mein Magen knurrt.\n",
    "Habe ich einen Durst!\n",
    "Jetzt wäre was zu trinken gut.\n",
    "Mein Bauch tut weh.\n",
    "\"\"\".split(\"\\n\")\n",
    "sentences = [s for s in sentences if s != '']\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"\n",
    "Meine Lieblingssprache ist Python.\n",
    "Ich programmiere gerne in Java.\n",
    "Webseiten werden mit HTML gestaltet.\n",
    "Für Data Science sind Jupyter Notebooks gut geeignet.\n",
    "BPNM dient zur Modellierung von Geschäftsprozessen\n",
    "\"\"\".split(\"\\n\")\n",
    "sentences = [s for s in sentences if s != '']\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ed4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "df_sim = pd.DataFrame(cos_sim.numpy(), index=sentences, columns=sentences)\n",
    "df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "# contradict, entail, neutral\n",
    "cmodel = CrossEncoder('cross-encoder/nli-deberta-v3-xsmall')\n",
    "# similarity\n",
    "# cmodel = CrossEncoder('cross-encoder/stsb-distilroberta-base')\n",
    "# deep passage retrieval\n",
    "# cmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05398dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = []\n",
    "for s1 in sentences:\n",
    "    s = []\n",
    "    for s2 in sentences:\n",
    "        s.append(cmodel.predict((s1, s2)))\n",
    "    sim.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c310f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Contradiction\")\n",
    "sns.heatmap(pd.DataFrame([[s[0] for s in row] for row in sim], index=sentences, columns=sentences),\n",
    "            annot=True, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e876f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Support\")\n",
    "sns.heatmap(pd.DataFrame([[s[1] for s in row] for row in sim], index=sentences, columns=sentences),\n",
    "            annot=True, cmap=\"Greens\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Neutral\")\n",
    "sns.heatmap(pd.DataFrame([[s[2] for s in row] for row in sim], index=sentences, columns=sentences),\n",
    "            annot=True, cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20eb965",
   "metadata": {},
   "source": [
    "## semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c06c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "heise = open(\"heise-newsticker.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6cd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bimodel = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0aae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "hembeddings = bimodel.encode(heise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3b2b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, util\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e34bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def search(query, text, corpus_embeddings, bi_encoder, cross_encoder, top_k=100):\n",
    "    # code query to restrict search space\n",
    "    question_embedding = bi_encoder.encode(query)\n",
    "    \n",
    "    # Determine similarity (vectors are normalized)\n",
    "    sim = np.dot(corpus_embeddings, question_embedding)\n",
    "    \n",
    "    # Get most similar top_k by sorting\n",
    "    hits = [ { \"text\": text[i], \"id\": i, \"score\": sim[i] } \n",
    "                     for i in sim.argsort()[::-1][0:top_k] ]\n",
    "\n",
    "    # Consider only top hits for re-rankin\n",
    "    cross_input = [[query, hit[\"text\"]] for hit in hits]\n",
    "    # cross-encode (this takes most time)\n",
    "    cross_scores = cross_encoder.predict(cross_input)\n",
    "\n",
    "    # Integrate cross-scores in original hits (this would be easier with pandas)\n",
    "    for i in range(len(cross_scores)):\n",
    "        hits[i][\"cross-score\"] = cross_scores[i]\n",
    "\n",
    "    # nre-sort by cross-score, descending!\n",
    "    hits = sorted(hits, key=lambda x: x[\"cross-score\"], reverse=True)\n",
    "    \n",
    "    # Return top-20 results of re-ranker as dataframe\n",
    "    return pd.DataFrame(hits[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c21779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize display in pandas\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5962948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>cross-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bard: Googles Antwort auf OpenAIs Chatbot ChatGPT\\n</td>\n",
       "      <td>17975</td>\n",
       "      <td>0.657352</td>\n",
       "      <td>1.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wie Wasserzeichen Texte von KI-Chatbots sichtbar machen könnten\\n</td>\n",
       "      <td>18167</td>\n",
       "      <td>0.559730</td>\n",
       "      <td>1.475290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI engagiert Programmierer, um ChatGPT im Coden zu verbessern \\n</td>\n",
       "      <td>15587</td>\n",
       "      <td>0.643041</td>\n",
       "      <td>-0.232210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Teams Premium: Abo mit KI-Diensten von OpenAI zum Schönrechnen \\n</td>\n",
       "      <td>18128</td>\n",
       "      <td>0.410450</td>\n",
       "      <td>-0.705258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#heiseshow: Chatbot LaMDA von Google – die erste KI mit einem Bewusstsein?\\n</td>\n",
       "      <td>6975</td>\n",
       "      <td>0.546075</td>\n",
       "      <td>-1.062128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Microsoft investiert weitere Milliarden in ChatGPT-Entwickler OpenAI\\n</td>\n",
       "      <td>15865</td>\n",
       "      <td>0.491298</td>\n",
       "      <td>-1.589022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chatbot LaMDA: Hat diese Google-Software wirklich ein Bewusstsein entwickelt?\\n</td>\n",
       "      <td>7170</td>\n",
       "      <td>0.541152</td>\n",
       "      <td>-2.019455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KI ChatGPT: Die wichtigsten Fragen und Antworten zum neuen Chatbot\\n</td>\n",
       "      <td>14893</td>\n",
       "      <td>0.593887</td>\n",
       "      <td>-2.354805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ChatGPT: Diese 10 Chatbot-Antworten machen tatsächlich Spaß\\n</td>\n",
       "      <td>15140</td>\n",
       "      <td>0.600820</td>\n",
       "      <td>-2.375225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ein Softwareentwickler kämpft für Persönlichkeitsrechte eines Chatbots\\n</td>\n",
       "      <td>9100</td>\n",
       "      <td>0.557956</td>\n",
       "      <td>-2.706526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kommentar: Google unterdrückt beim Chatbot LaMDA eine wichtige Debatte\\n</td>\n",
       "      <td>7241</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>-2.949592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Generative KI: OpenAI &amp; Co. unterzeichnen Regelwerk für Ethik und Transparenz \\n</td>\n",
       "      <td>16966</td>\n",
       "      <td>0.415549</td>\n",
       "      <td>-3.092259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chatbot-Suche: Presseverlage verlangen Lizenzgebühren von Google und Microsoft\\n</td>\n",
       "      <td>17666</td>\n",
       "      <td>0.528215</td>\n",
       "      <td>-3.700642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GPTZero: App soll von KI wie ChatGPT geschriebene Texte erkennen können\\n</td>\n",
       "      <td>16595</td>\n",
       "      <td>0.419569</td>\n",
       "      <td>-3.776809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hat Chatbot LaMDA ein Bewusstein entwickelt? Google beurlaubt Angestellten\\n</td>\n",
       "      <td>7441</td>\n",
       "      <td>0.535859</td>\n",
       "      <td>-3.839421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Baidu veröffentlicht ErnieBot – ChatGPT-Konkurrenz für seine Suchmaschine\\n</td>\n",
       "      <td>17951</td>\n",
       "      <td>0.426346</td>\n",
       "      <td>-3.839816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>heise meets …Chatbots werden sich durchsetzen – Arbeitgeber müssen reagieren\\n</td>\n",
       "      <td>17136</td>\n",
       "      <td>0.502378</td>\n",
       "      <td>-4.078547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Künstliche Intelligenz: Microsoft bringt Chatbot aufs Smartphone und für Skype\\n</td>\n",
       "      <td>17227</td>\n",
       "      <td>0.494122</td>\n",
       "      <td>-4.221371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Amazon schaltet automatischen Chatbot-Designer in Amazon Lex frei\\n</td>\n",
       "      <td>7052</td>\n",
       "      <td>0.480444</td>\n",
       "      <td>-4.403298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Russland blockiert Onlineangebote von Deutscher Welle und BBC \\n</td>\n",
       "      <td>3837</td>\n",
       "      <td>0.390109</td>\n",
       "      <td>-4.627187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                text  \\\n",
       "0                                Bard: Googles Antwort auf OpenAIs Chatbot ChatGPT\\n   \n",
       "1                  Wie Wasserzeichen Texte von KI-Chatbots sichtbar machen könnten\\n   \n",
       "2               OpenAI engagiert Programmierer, um ChatGPT im Coden zu verbessern \\n   \n",
       "3        Microsoft Teams Premium: Abo mit KI-Diensten von OpenAI zum Schönrechnen \\n   \n",
       "4       #heiseshow: Chatbot LaMDA von Google – die erste KI mit einem Bewusstsein?\\n   \n",
       "5             Microsoft investiert weitere Milliarden in ChatGPT-Entwickler OpenAI\\n   \n",
       "6    Chatbot LaMDA: Hat diese Google-Software wirklich ein Bewusstsein entwickelt?\\n   \n",
       "7               KI ChatGPT: Die wichtigsten Fragen und Antworten zum neuen Chatbot\\n   \n",
       "8                      ChatGPT: Diese 10 Chatbot-Antworten machen tatsächlich Spaß\\n   \n",
       "9           Ein Softwareentwickler kämpft für Persönlichkeitsrechte eines Chatbots\\n   \n",
       "10          Kommentar: Google unterdrückt beim Chatbot LaMDA eine wichtige Debatte\\n   \n",
       "11  Generative KI: OpenAI & Co. unterzeichnen Regelwerk für Ethik und Transparenz \\n   \n",
       "12  Chatbot-Suche: Presseverlage verlangen Lizenzgebühren von Google und Microsoft\\n   \n",
       "13         GPTZero: App soll von KI wie ChatGPT geschriebene Texte erkennen können\\n   \n",
       "14      Hat Chatbot LaMDA ein Bewusstein entwickelt? Google beurlaubt Angestellten\\n   \n",
       "15       Baidu veröffentlicht ErnieBot – ChatGPT-Konkurrenz für seine Suchmaschine\\n   \n",
       "16    heise meets …Chatbots werden sich durchsetzen – Arbeitgeber müssen reagieren\\n   \n",
       "17  Künstliche Intelligenz: Microsoft bringt Chatbot aufs Smartphone und für Skype\\n   \n",
       "18               Amazon schaltet automatischen Chatbot-Designer in Amazon Lex frei\\n   \n",
       "19                  Russland blockiert Onlineangebote von Deutscher Welle und BBC \\n   \n",
       "\n",
       "       id     score  cross-score  \n",
       "0   17975  0.657352     1.520100  \n",
       "1   18167  0.559730     1.475290  \n",
       "2   15587  0.643041    -0.232210  \n",
       "3   18128  0.410450    -0.705258  \n",
       "4    6975  0.546075    -1.062128  \n",
       "5   15865  0.491298    -1.589022  \n",
       "6    7170  0.541152    -2.019455  \n",
       "7   14893  0.593887    -2.354805  \n",
       "8   15140  0.600820    -2.375225  \n",
       "9    9100  0.557956    -2.706526  \n",
       "10   7241  0.605860    -2.949592  \n",
       "11  16966  0.415549    -3.092259  \n",
       "12  17666  0.528215    -3.700642  \n",
       "13  16595  0.419569    -3.776809  \n",
       "14   7441  0.535859    -3.839421  \n",
       "15  17951  0.426346    -3.839816  \n",
       "16  17136  0.502378    -4.078547  \n",
       "17  17227  0.494122    -4.221371  \n",
       "18   7052  0.480444    -4.403298  \n",
       "19   3837  0.390109    -4.627187  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"Wie heißt der Chatbot von OpenAI?\", \n",
    "       heise, hembeddings, bimodel, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02c8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52542784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdaff683",
   "metadata": {},
   "source": [
    "# txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Extractor\n",
    "\n",
    "# Create embeddings model, backed by sentence-transformers & transformers\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n",
    "\n",
    "# Create extractor instance\n",
    "extractor = Extractor(embeddings, \"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Giants hit 3 HRs to down Dodgers\",\n",
    "        \"Giants 5 Dodgers 4 final\",\n",
    "        \"Dodgers drop Game 2 against the Giants, 5-4\",\n",
    "        \"Blue Jays beat Red Sox final score 2-1\",\n",
    "        \"Red Sox lost to the Blue Jays, 2-1\",\n",
    "        \"Blue Jays at Red Sox is over. Score: 2-1\",\n",
    "        \"Phillies win over the Braves, 5-0\",\n",
    "        \"Phillies 5 Braves 0 final\",\n",
    "        \"Final: Braves lose to the Phillies in the series opener, 5-0\",\n",
    "        \"Lightning goaltender pulled, lose to Flyers 4-1\",\n",
    "        \"Flyers 4 Lightning 1 final\",\n",
    "        \"Flyers win 4-1\"]\n",
    "\n",
    "questions = [\"What team won the game?\", \"What was score?\"]\n",
    "\n",
    "execute = lambda query: extractor([(question, query, question, False) for question in questions], data)\n",
    "\n",
    "for query in [\"Red Sox - Blue Jays\", \"Phillies - Braves\", \"Dodgers - Giants\", \"Flyers - Lightning\"]:\n",
    "    print(\"----\", query, \"----\")\n",
    "    for answer in execute(query):\n",
    "        print(answer)\n",
    "    print()\n",
    "\n",
    "# Ad-hoc questions\n",
    "question = \"What hockey team won?\"\n",
    "\n",
    "print(\"----\", question, \"----\")\n",
    "print(extractor([(question, question, question, False)], data))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor([[\"What team won?\"]*3+[False]], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f297f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "heise = open(\"heise-newsticker.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor([[\"Wie viel Umsatz macht Microsoft\"]*3+[False]], heise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f3020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
